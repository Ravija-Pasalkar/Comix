# -*- coding: utf-8 -*-
"""smallpox.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DCq7PIDl8-zr-bgdPoRAJa-9IYzxvHTA
"""

# Step 1: Setup & Drive Mounting
from google.colab import drive
import os

drive.mount('/content/drive')
VIDEO_DIR = '/content/drive/MyDrive/content/videos'
os.makedirs(VIDEO_DIR, exist_ok=True)

import cv2
import os

def extract_frames(video_path, output_dir, frame_interval=2):
    os.makedirs(output_dir, exist_ok=True)
    cap = cv2.VideoCapture(video_path)
    frame_count = 0
    extracted_count = 0

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        if frame_count % frame_interval == 0:
            frame_filename = os.path.join(output_dir, f'frame_{extracted_count:04d}.jpg')
            cv2.imwrite(frame_filename, frame)
            extracted_count += 1

        frame_count += 1

    cap.release()
    print(f'Extracted {extracted_count} frames from {video_path}')
    print(f'Frames saved in {output_dir}')

video_path = os.path.join(VIDEO_DIR, 'smallpox.mp4')
output_dir = os.path.join(VIDEO_DIR, 'frames')
extract_frames(video_path, output_dir)

import torch
import torchvision.models as models
import torchvision.transforms as transforms
from PIL import Image
import numpy as np
import os
import glob

# Load ResNet-50 (Pretrained)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
resnet = models.resnet50(pretrained=True)
resnet = torch.nn.Sequential(*list(resnet.children())[:-1])
resnet.to(device)
resnet.eval()

# Image Transformations
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

def extract_features(image_path):
    """Extract features from an image using ResNet and return a 2048-dimensional vector."""
    img = Image.open(image_path).convert("RGB")
    img = transform(img).unsqueeze(0).to(device)  # Add batch dimension (1, 3, 224, 224)

    with torch.no_grad():
        features = resnet(img)  # Output shape: (1, 2048, 1, 1)

    return features.squeeze().cpu().numpy()  # Final shape: (2048,)

# Paths
frames_dir = "/content/drive/MyDrive/content/videos/frames"
features_dir = "/content/drive/MyDrive/content/videos/features"

# Create directory for saving features if it doesn't exist
os.makedirs(features_dir, exist_ok=True)

# Process all frames
frame_files = sorted(glob.glob(os.path.join(frames_dir, "*.jpg")))

for frame_path in frame_files:
    features = extract_features(frame_path)
    filename = os.path.basename(frame_path).replace(".jpg", ".npy")
    np.save(os.path.join(features_dir, filename), features)

print(f"Feature extraction completed. Features saved in {features_dir}")

import cv2
import numpy as np
import os
import glob

def compute_brightness(image_path):
    """Compute brightness score of an image (normalized to [0,1])."""
    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)  # Read as grayscale
    return np.mean(img) / 255.0

def compute_motion(prev_img, curr_img):
    """Compute motion score between two consecutive grayscale frames (normalized to [0,1])."""
    diff = cv2.absdiff(prev_img, curr_img)  # Pixel-wise absolute difference
    return np.mean(diff) / 255.0

def compute_highlight_scores(frames_dir):
    """Compute highlight scores for frames using brightness and motion."""
    frame_files = sorted(glob.glob(os.path.join(frames_dir, "*.jpg")))
    highlight_scores = {}
    prev_frame = None  # Store previous frame for motion computation

    for i, frame_path in enumerate(frame_files):
        frame_name = os.path.basename(frame_path).replace(".jpg", "")
        curr_frame = cv2.imread(frame_path, cv2.IMREAD_GRAYSCALE)  # Read frame as grayscale

        # Compute brightness score
        brightness_score = compute_brightness(frame_path)

        # Compute motion score (skip for first frame)
        motion_score = compute_motion(prev_frame, curr_frame) if prev_frame is not None else 0
        prev_frame = curr_frame  # Update previous frame

        # Compute final highlight score (weighted sum)
        highlight_scores[frame_name] = (0.7 * brightness_score) + (0.3 * motion_score)

    return highlight_scores

def save_highlight_scores(highlight_scores, labels_file):
    """Save highlight scores to a text file."""
    with open(labels_file, "w") as f:
        for frame_name, score in highlight_scores.items():
            f.write(f"{frame_name} {score:.4f}\n")
    print(f"Pseudo-labels (brightness + motion) saved in {labels_file}")

# Paths
frames_dir = "/content/drive/MyDrive/content/videos/frames"
labels_file = "/content/drive/MyDrive/content/videos/highlight_labels.txt"

# Compute and save highlight scores
highlight_scores = compute_highlight_scores(frames_dir)
save_highlight_scores(highlight_scores, labels_file)

sample_feature = np.load(sorted(glob.glob(os.path.join(features_dir, "*.npy")))[0])
print("Updated Feature Shape:", sample_feature.shape)  # Should be (2048,)

import numpy as np
import os
import glob
import torch
from sklearn.model_selection import train_test_split

def load_features(features_dir):
    """Load frame features from .npy files and return as a NumPy array."""
    frame_files = sorted(glob.glob(os.path.join(features_dir, "*.npy")))
    frame_features = [np.load(f) for f in frame_files]
    frame_features = np.vstack(frame_features)  # Shape: (num_frames, 2048)
    frame_names = [os.path.basename(f).replace(".npy", "") for f in frame_files]
    return frame_features, frame_names

def load_highlight_scores(labels_file):
    """Load highlight scores from the text file and return as a dictionary."""
    highlight_scores = {}
    with open(labels_file, "r") as f:
        for line in f:
            frame, score = line.strip().split()
            highlight_scores[frame] = float(score)
    return highlight_scores

def prepare_data(features, frame_names, highlight_scores, test_size=0.2):
    """Align features with highlight scores and split into train/test sets."""
    y = np.array([highlight_scores[name] for name in frame_names])
    X_train, X_test, y_train, y_test = train_test_split(features, y, test_size=test_size, random_state=42)

    # Convert to PyTorch tensors
    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)  # Shape (N,1)
    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
    y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)

    return X_train_tensor, X_test_tensor, y_train_tensor, y_test_tensor

# Paths
features_dir = "/content/drive/MyDrive/content/videos/features"
labels_file = "/content/drive/MyDrive/content/videos/highlight_labels.txt"

# Load data
features, frame_names = load_features(features_dir)
highlight_scores = load_highlight_scores(labels_file)

# Prepare dataset
X_train_tensor, X_test_tensor, y_train_tensor, y_test_tensor = prepare_data(features, frame_names, highlight_scores)

print("Data preparation completed. Ready for model training.")

import torch.nn as nn
import torch.optim as optim
import torch
import os

# Model Save Path
model_save_path = "/content/drive/MyDrive/content/models/highlight_model.pth"
os.makedirs(os.path.dirname(model_save_path), exist_ok=True)

class HighlightScoringModel(nn.Module):
    def __init__(self, input_dim=2048, hidden_dim=512):
        super(HighlightScoringModel, self).__init__()
        self.fc = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1),
            nn.Sigmoid()  # Outputs scores between 0 and 1
        )

    def forward(self, x):
        return self.fc(x)

def train_model(X_train_tensor, y_train_tensor, num_epochs=50, batch_size=32, learning_rate=0.001):
    """Train the highlight scoring model."""
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = HighlightScoringModel().to(device)

    criterion = nn.MSELoss()  # Regression loss
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)

    X_train_tensor, y_train_tensor = X_train_tensor.to(device), y_train_tensor.to(device)

    for epoch in range(num_epochs):
        model.train()
        optimizer.zero_grad()

        predictions = model(X_train_tensor)
        loss = criterion(predictions, y_train_tensor)

        loss.backward()
        optimizer.step()

        if (epoch + 1) % 10 == 0:
            print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}")

    torch.save(model.state_dict(), model_save_path)
    print(f"Model saved to {model_save_path}")

    return model

# Example usage:
model = train_model(X_train_tensor, y_train_tensor)

import torch
import numpy as np
import os
import glob

# Load trained model
def load_model(model, model_path, device):
    model.load_state_dict(torch.load(model_path, map_location=device))
    model.eval()
    return model

# Compute highlightness scores
def compute_highlight_scores(model, features_dir, highlight_scores_dir, device):
    os.makedirs(highlight_scores_dir, exist_ok=True)
    frame_files = sorted(glob.glob(os.path.join(features_dir, "*.npy")))
    highlight_scores = {}

    for feature_path in frame_files:
        features = np.load(feature_path)
        features_tensor = torch.tensor(features, dtype=torch.float32).to(device).unsqueeze(0)  # Add batch dimension

        with torch.no_grad():
            score = model(features_tensor).item()

        filename = os.path.basename(feature_path).replace(".npy", ".txt")
        highlight_scores[filename] = score

        with open(os.path.join(highlight_scores_dir, filename), "w") as f:
            f.write(str(score))

    print(f"Highlight scores saved in {highlight_scores_dir}")
    return highlight_scores

# Paths
model_save_path = "/content/drive/MyDrive/content/models/highlight_model.pth"
features_dir = "/content/drive/MyDrive/content/videos/features"
highlight_scores_dir = "/content/drive/MyDrive/content/videos/highlight_scores"

# Load device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Load model
model = HighlightScoringModel().to(device)
model = load_model(model, model_save_path, device)

# Compute scores
highlight_scores = compute_highlight_scores(model, features_dir, highlight_scores_dir, device)

!pip install scenedetect

import os
from scenedetect import VideoManager, SceneManager
from scenedetect.detectors import ContentDetector

def detect_scenes(video_path, output_file, threshold=20):
    # Initialize Video Manager and Scene Manager
    video_manager = VideoManager([video_path])
    scene_manager = SceneManager()
    scene_manager.add_detector(ContentDetector(threshold=threshold))

    # Process video
    video_manager.set_downscale_factor()
    video_manager.start()
    scene_manager.detect_scenes(frame_source=video_manager)

    # Get scene list
    scene_list = scene_manager.get_scene_list()
    fps = video_manager.get_framerate()  # Get FPS

    # Save segmentation points
    with open(segments_output_file, "w") as f:
        for start_time, end_time in scene_list:
            start_frame = int(start_time.get_seconds() * fps)
            end_frame = int(end_time.get_seconds() * fps)
            f.write(f"{start_frame} {end_frame}\n")

    print(f"Temporal segmentation completed. Segments saved in {output_file}")
    return scene_list

# Paths
video_path = "/content/drive/MyDrive/content/videos/smallpox.mp4"
segments_output_file = "/content/drive/MyDrive/content/videos/segments.txt"

# Run scene detection
scene_list = detect_scenes(video_path, segments_output_file)

import os
import numpy as np
import cv2
from skimage.metrics import structural_similarity as ssim

def load_highlight_scores(file_path):
    """Load highlight scores from a text file."""
    scores = {}
    with open(file_path, "r") as f:
        for line in f:
            frame_name, score = line.strip().split()
            scores[frame_name.replace(".jpg", "")] = float(score)
    return scores

def load_segments(file_path):
    """Load segments from a text file."""
    segments = []
    with open(file_path, "r") as f:
        for line in f:
            start_frame, end_frame = map(int, line.strip().split())
            segments.append((start_frame, end_frame))
    return segments

def compute_ssim(frame1, frame2):
    """Compute Structural Similarity Index (SSIM) between two frames."""
    frame1 = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)
    frame2 = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)
    return ssim(frame1, frame2)

def is_opaque_frame(frame, threshold=10):
    """Check if a frame is nearly a single color (fully opaque)."""
    avg_color = np.mean(frame, axis=(0, 1))
    max_diff = np.max(np.abs(frame - avg_color))
    return max_diff < threshold

def select_keyframes(segments, highlight_scores, frames_dir, N=2, similarity_threshold=0.555, min_gap=5):
    """Selects keyframes from segments based on highlight scores, visual diversity, and opacity checks."""
    selected_keyframes = []

    for start, end in segments:
        scores = [(f"frame_{i:04d}", highlight_scores[f"frame_{i:04d}"])
                  for i in range(start, end + 1) if f"frame_{i:04d}" in highlight_scores]

        scores.sort(key=lambda x: (-x[1], int(x[0].split("_")[1])))

        selected = []
        last_selected_index = -min_gap - 1

        for frame_name, _ in scores:
            frame_index = int(frame_name.split("_")[1])

            if frame_index - last_selected_index <= min_gap:
                continue

            frame_path = os.path.join(frames_dir, frame_name + ".jpg")
            if not os.path.exists(frame_path):
                continue

            frame = cv2.imread(frame_path)

            if is_opaque_frame(frame):
                continue

            is_similar = any(
                compute_ssim(frame, cv2.imread(os.path.join(frames_dir, sf + ".jpg"))) > similarity_threshold
                for sf in selected if os.path.exists(os.path.join(frames_dir, sf + ".jpg"))
            )

            if not is_similar:
                selected.append(frame_name)
                last_selected_index = frame_index

            if len(selected) >= N:
                break

        selected_keyframes.extend(selected)

    selected_keyframes.sort(key=lambda x: int(x.split("_")[1]))
    return selected_keyframes

def save_keyframes(file_path, keyframes):
    """Save selected keyframes to a text file."""
    with open(file_path, "w") as f:
        for keyframe in keyframes:
            f.write(f"{keyframe}.jpg\n")

# Paths
highlight_scores_file = "/content/drive/MyDrive/content/videos/highlight_labels.txt"
segments_file = "/content/drive/MyDrive/content/videos/segments.txt"
frames_dir = "/content/drive/MyDrive/content/videos/frames"
keyframes_output_file = "/content/drive/MyDrive/content/videos/selected_keyframes.txt"

# Load data
highlight_scores = load_highlight_scores(highlight_scores_file)
segments = load_segments(segments_file)

# Select and save keyframes
selected_keyframes = select_keyframes(segments, highlight_scores, frames_dir)
save_keyframes(keyframes_output_file, selected_keyframes)

print(f"Keyframe selection completed. Keyframes saved at {keyframes_output_file}")

import os
import matplotlib.pyplot as plt
import cv2
import math

# Paths
frames_dir = "/content/drive/MyDrive/content/videos/frames"
keyframes_file = "/content/drive/MyDrive/content/videos/selected_keyframes.txt"

def load_keyframes(file_path):
    """Load selected keyframes from a file."""
    with open(file_path, "r") as f:
        return [line.strip() for line in f.readlines()]

def display_keyframes(frames_dir, keyframes, cols=5):
    """Display keyframes in a grid layout."""
    num_keyframes = len(keyframes)
    rows = math.ceil(num_keyframes / cols)

    plt.figure(figsize=(cols * 3, rows * 3))

    for idx, keyframe in enumerate(keyframes):
        frame_path = os.path.join(frames_dir, keyframe)

        if os.path.exists(frame_path):
            img = cv2.imread(frame_path)
            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

            plt.subplot(rows, cols, idx + 1)
            plt.imshow(img)
            plt.axis("off")
            plt.title(keyframe)
        else:
            print(f"Frame not found: {frame_path}")

    plt.tight_layout()
    plt.show()

def main():
    keyframes = load_keyframes(keyframes_file)
    display_keyframes(frames_dir, keyframes)

if __name__ == "__main__":
    main()

!pip install tensorflow keras timm

import tensorflow as tf
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.layers import GlobalAveragePooling2D, Dense
from tensorflow.keras.models import Model

def build_aesthetic_model():
    """Creates a ResNet50-based model for aesthetic score prediction."""
    base_model = ResNet50(weights="imagenet", include_top=False)
    x = GlobalAveragePooling2D()(base_model.output)
    x = Dense(1, activation="linear")(x)  # Single neuron for aesthetic score prediction
    model = Model(inputs=base_model.input, outputs=x)
    return model

aesthetic_model = build_aesthetic_model()

import numpy as np
import cv2

def predict_aesthetic_score(model, image_path):
    """Predicts aesthetic score using ResNet50 trained on AVA dataset."""
    img = cv2.imread(image_path)
    img = cv2.resize(img, (224, 224))  # Resize for ResNet50
    img = img.astype(np.float32) / 255.0  # Normalize
    img = np.expand_dims(img, axis=0)  # Add batch dimension

    score = aesthetic_model.predict(img)[0][0]
    return score

import os
import re

def load_keyframes(file_path):
    """Loads keyframe names from a text file."""
    with open(file_path, "r") as f:
        return [line.strip() for line in f]

def compute_aesthetic_scores(keyframes, frames_dir, model):
    """Computes aesthetic scores for given keyframes."""
    scores = []
    for frame_name in keyframes:
        frame_path = os.path.join(frames_dir, frame_name)
        if os.path.exists(frame_path):
            score = predict_aesthetic_score(model, frame_path)
            scores.append((frame_name, score))
    return scores

def save_final_keyframes(scores, output_file, N=30):
    """Saves top N aesthetic keyframes to a file (sorted by frame number)."""
    scores.sort(key=lambda x: -x[1])  # Sort by highest aesthetic score
    final_keyframes = [frame[0] for frame in scores[:N]]
    final_keyframes.sort(key=lambda x: int(re.search(r'\d+', x).group()))  # Sort numerically

    with open(output_file, "w") as f:
        for keyframe in final_keyframes:
            f.write(f"{keyframe}\n")

    print(f"Aesthetic estimation completed. Final keyframes saved in {output_file}")

# Paths
frames_dir = "/content/drive/MyDrive/content/videos/frames"
keyframes_file = "/content/drive/MyDrive/content/videos/selected_keyframes.txt"
aesthetic_output_file = "/content/drive/MyDrive/content/videos/aesthetic_keyframes.txt"

# Load keyframes
keyframes = load_keyframes(keyframes_file)

# Compute aesthetic scores
aesthetic_scores = compute_aesthetic_scores(keyframes, frames_dir, aesthetic_model)

# Save final keyframes
save_final_keyframes(aesthetic_scores, aesthetic_output_file, N=30)

frames_dir = "/content/drive/MyDrive/content/videos/frames"
aesthetic_keyframes_file = "/content/drive/MyDrive/content/videos/aesthetic_keyframes.txt"

# Load and display keyframes
keyframes = load_keyframes(aesthetic_keyframes_file)
display_keyframes(frames_dir, keyframes)

!pip install torch torchvision numpy opencv-python

# Commented out IPython magic to ensure Python compatibility.
!git clone https://github.com/Yijunmaverick/CartoonGAN-Test-Pytorch-Torch.git
# %cd CartoonGAN-Test-Pytorch-Torch

# Commented out IPython magic to ensure Python compatibility.
!mkdir -p pretrained_model
# %cd pretrained_model
!wget -c http://vllab1.ucmerced.edu/~yli62/CartoonGAN/pytorch_pth/Hayao_net_G_float.pth
!wget -c http://vllab1.ucmerced.edu/~yli62/CartoonGAN/pytorch_pth/Hosoda_net_G_float.pth
!wget -c http://vllab1.ucmerced.edu/~yli62/CartoonGAN/pytorch_pth/Paprika_net_G_float.pth
!wget -c http://vllab1.ucmerced.edu/~yli62/CartoonGAN/pytorch_pth/Shinkai_net_G_float.pth
# %cd ..

!ls pretrained_model

import os
import cv2
import torch
import numpy as np
from tqdm import tqdm
from PIL import Image
import torchvision.transforms as transforms
import matplotlib.pyplot as plt
from network.Transformer import Transformer

# Paths
frames_dir = "/content/drive/MyDrive/content/videos/frames"
aesthetic_keyframes_file = "/content/drive/MyDrive/content/videos/aesthetic_keyframes.txt"
output_dir = "/content/drive/MyDrive/content/videos/stylized_temp"
os.makedirs(output_dir, exist_ok=True)

# Load Pretrained Style Transfer Model
def load_style_transfer_model(model_path="pretrained_model/Hayao_net_G_float.pth"):
    model = Transformer()
    model.load_state_dict(torch.load(model_path))
    model.eval()
    return model

# Image Processing Functions
def smooth_stylization(image, d=15, sigmaColor=75, sigmaSpace=75):
    return cv2.bilateralFilter(image, d, sigmaColor, sigmaSpace)

def sharpen_edges(image):
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    edges = cv2.Canny(gray, 80, 220)
    edges_colored = cv2.cvtColor(edges, cv2.COLOR_GRAY2BGR)
    return cv2.addWeighted(image, 0.85, edges_colored, 0.3, 0)

def refine_outlines(image):
    blurred = cv2.GaussianBlur(image, (3, 3), 0)
    laplacian = cv2.Laplacian(blurred, cv2.CV_8U, ksize=3)
    if len(laplacian.shape) == 2:
        laplacian = cv2.cvtColor(laplacian, cv2.COLOR_GRAY2BGR)
    return cv2.addWeighted(image, 1.0, laplacian, -0.5, 0)

def apply_stylization(image):
    smooth = smooth_stylization(image)
    sharpened = sharpen_edges(smooth)
    outlined = refine_outlines(sharpened)
    return outlined

# Load Keyframes
def load_keyframes(file_path):
    with open(file_path, "r") as file:
        return [line.strip() for line in file.readlines()]

# Process and Save Stylized Keyframes
def process_keyframes():
    keyframes = load_keyframes(aesthetic_keyframes_file)
    for filename in tqdm(keyframes, desc="Processing keyframes"):
        img_path = os.path.join(frames_dir, filename)
        if not os.path.exists(img_path):
            print(f"Skipping {filename}, file not found.")
            continue

        image = cv2.imread(img_path)
        if image is None:
            print(f"Error loading image: {img_path}")
            continue

        stylized = apply_stylization(image)
        output_path = os.path.join(output_dir, filename)
        cv2.imwrite(output_path, stylized)

    print("Style transfer complete.")

if __name__ == "__main__":
    process_keyframes()

import cv2
import matplotlib.pyplot as plt
import os

def display_stylized_images(output_dir):
    """Displays all stylized images stored in the output directory."""
    filenames = sorted(os.listdir(output_dir))
    num_images = len(filenames)

    if num_images == 0:
        print("No images found in the directory.")
        return

    rows = (num_images + 4) // 5  # Arrange in rows of 5
    plt.figure(figsize=(15, rows * 3))

    for i, filename in enumerate(filenames):
        img_path = os.path.join(output_dir, filename)
        img = cv2.imread(img_path)

        if img is None:
            print(f"Error loading image: {filename}")
            continue

        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        plt.subplot(rows, 5, i + 1)
        plt.imshow(img)
        plt.title(filename, fontsize=8)
        plt.axis("off")

    plt.tight_layout()
    plt.show()

output_dir = "/content/drive/MyDrive/content/videos/stylized_temp"
display_stylized_images(output_dir)

!pip install -q git+https://github.com/openai/whisper.git
!pip install -q ffmpeg

video_path = "/content/drive/MyDrive/content/videos/smallpox.mp4"
audio_path = "/content/drive/MyDrive/content/audio/smallpox.mp4.wav"
text_output_path = "/content/drive/MyDrive/content/audio/smallpox.json"
aesthetic_keyframes_file = "/content/drive/MyDrive/content/videos/aesthetic_keyframes.txt"
summarized_text = "/content/drive/MyDrive/content/audio/summarized_transcript.json"
comic_text_path = "/content/drive/MyDrive/content/audio/comic_text.json"

import subprocess
import os
import json
import whisper

def extract_audio(video_path, audio_path):
    """Extracts audio from a video file using FFmpeg."""
    os.makedirs(os.path.dirname(audio_path), exist_ok=True)
    command = f"ffmpeg -i '{video_path}' -q:a 0 -map a '{audio_path}' -y"
    result = subprocess.run(command, shell=True, capture_output=True, text=True)
    return True

def transcribe_audio(audio_path, text_output_path, model_size="base"):
    """Transcribes audio using OpenAI's Whisper model."""
    os.makedirs(os.path.dirname(text_output_path), exist_ok=True)
    model = whisper.load_model(model_size)
    result = model.transcribe(audio_path)

    with open(text_output_path, "w") as f:
        json.dump(result["segments"], f, indent=4)

    print("Transcription saved at:", text_output_path)

def load_keyframes(aesthetic_keyframes_file):
    """Loads and sorts keyframes based on frame index."""
    with open(aesthetic_keyframes_file, "r") as f:
        return sorted([int(line.strip().split("_")[1].split(".")[0]) for line in f.readlines()])


if extract_audio(video_path, audio_path):
    transcribe_audio(audio_path, text_output_path)

keyframes = load_keyframes(aesthetic_keyframes_file)
print("Loaded keyframes:", keyframes)

from transformers import pipeline
import json

def summarize_transcript(text_output_path, keyframes, summarized_text, model_name="facebook/bart-large-cnn"):
    """Summarizes dialogue in video segments based on keyframes."""
    summarizer = pipeline("summarization", model=model_name)

    with open(text_output_path, "r") as f:
        transcript = json.load(f)

    summarized_transcript = {}

    for i in range(len(keyframes) - 1):
        segment_text = " ".join([
            seg["text"] for seg in transcript
            if keyframes[i] <= (seg["start"] * 30 / 2) < keyframes[i+1]  # Convert Whisper time to frames
        ])

        summary = summarizer(segment_text, max_length=50, min_length=20, do_sample=False)[0]["summary_text"] if segment_text.strip() else ""

        summarized_transcript[f"Segment {i+1}"] = summary

    with open(summarized_text, "w") as f:
        json.dump(summarized_transcript, f, indent=4)

    print("Summarization complete. Saved to:", summarized_text)

# Example usage
text_output_path = "/content/drive/MyDrive/content/audio/smallpox.json"
summarized_text = "/content/drive/MyDrive/content/audio/summarized_transcript.json"

summarize_transcript(text_output_path, keyframes, summarized_text)

!pip install matplotlib pillow reportlab

!pip install transformers sentence-transformers torch torchvision

import os
import json
import cv2
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image, ImageDraw, ImageFont
from reportlab.pdfgen import canvas

# Paths
stylized_frames_dir = "/content/drive/MyDrive/content/videos/stylized_temp"
summarized_text = "/content/drive/MyDrive/content/audio/summarized_transcript.json"
comic_pdf_path = "/content/drive/MyDrive/content/comix_output.pdf"

# Load Summarized Text
with open(summarized_text, "r") as f:
    summarized_text = json.load(f)

frame_files = sorted(os.listdir(stylized_frames_dir))
styled_images = [os.path.join(stylized_frames_dir, file) for file in frame_files]

# Comic Layout Parameters
page_width, page_height = 1400, 1000  # A4 Landscape
max_panels_per_page = 6  # Each page contains 5-6 panels


# Load Comic Font (Ensure you have a comic-style font installed)
font_path = "/content/drive/MyDrive/content/font/digistrip.ttf"  # Change if needed
caption_font = ImageFont.truetype(font_path, 24)

def wrap_text(text, font, max_width):
    lines = []
    words = text.split(" ")
    current_line = words[0]

    for word in words[1:]:
        test_line = current_line + " " + word
        bbox = font.getbbox(test_line)
        width = bbox[2] - bbox[0]

        if width <= max_width:
            current_line = test_line
        else:
            lines.append(current_line)
            current_line = word

    lines.append(current_line)
    return lines

# Function to Create a Comic Page
def create_comic_page(image_paths, output_path, page_number, segment_index):
    page = Image.new('RGB', (page_width, page_height), 'white')
    draw = ImageDraw.Draw(page)

    # Panel Layouts for 5 & 6 Panels
    if len(image_paths) == 5:
        panel_positions = [
            (50, 50, 900, 300),
            (920, 50, 1350, 300),
            (50, 320, 450, 600),
            (470, 320, 1350, 600),
            (50, 620, 1350, 950),
        ]
    else:
        panel_positions = [
            (50, 50, 650, 300),
            (670, 50, 1350, 300),
            (50, 320, 450, 600),
            (470, 320, 1350, 600),
            (50, 620, 650, 950),
            (670, 620, 1350, 950),
        ]

    for idx, img_path in enumerate(image_paths):
        x1, y1, x2, y2 = panel_positions[idx]
        width, height = x2 - x1, y2 - y1

        img = Image.open(img_path).resize((width, height))
        page.paste(img, (x1, y1))

        draw.rectangle([x1, y1, x2, y2], outline="black", width=5)

        # Add Speech Bubble
        caption = summarized_text.get(f"Segment {segment_index + 1}", "")
        if caption.strip():
            caption_lines = wrap_text(caption, caption_font, width - 20)
            caption_height = len(caption_lines) * 30

            caption_box = (x1, y2 - caption_height, x2, y2)
            draw.rectangle(caption_box, fill=(255, 255, 255), outline="black", width=3)

            for i, line in enumerate(caption_lines):
                draw.text((x1 + 10, y2 - caption_height + (i * 30)), line, fill="black", font=caption_font)

        segment_index += 1

    # Page Number
    draw.rectangle([page_width - 120, page_height - 60, page_width - 20, page_height - 20], fill='black')
    draw.text((page_width - 100, page_height - 50), f"{page_number}", fill="white", font=caption_font)

    page.save(output_path)
    return segment_index

# Generate Comic Pages & Save PDF
comic_pages = []
segment_index = 0

for i in range(0, len(styled_images), max_panels_per_page):
    page_images = styled_images[i:i+6] if len(styled_images[i:i+6]) == 6 else styled_images[i:i+5]
    page_path = f"/content/drive/MyDrive/content/comic_page_{i//max_panels_per_page}.jpg"
    segment_index = create_comic_page(page_images, page_path, (i//max_panels_per_page) + 1, segment_index)
    comic_pages.append(Image.open(page_path))

if comic_pages:
    comic_pages[0].save(comic_pdf_path, save_all=True, append_images=comic_pages[1:])
    print(f"Comic book PDF saved at: {comic_pdf_path}")
else:
    print("No images found to create a PDF!")